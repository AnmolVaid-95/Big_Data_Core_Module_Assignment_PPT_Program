{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.**"
      ],
      "metadata": {
        "id": "8_aNAczZUP92"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6BrCDHbUHUD"
      },
      "outputs": [],
      "source": [
        "import configparser\n",
        "\n",
        "def display_core_components():\n",
        "    # Create a ConfigParser object\n",
        "    config = configparser.ConfigParser()\n",
        "\n",
        "    # Read the Hadoop configuration file\n",
        "    config.read('path/to/hadoop/conf/hadoop-core-site.xml')\n",
        "\n",
        "    # Get the core components from the configuration\n",
        "    core_components = config.get('core', 'fs.defaultFS')\n",
        "\n",
        "    print(\"Core Components of Hadoop:\")\n",
        "    print(core_components)\n",
        "\n",
        "\n",
        "# Call the function to display the core components\n",
        "display_core_components()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.**"
      ],
      "metadata": {
        "id": "vX484kebVK_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hdfs import InsecureClient\n",
        "\n",
        "def calculate_total_file_size(hdfs_url, directory_path):\n",
        "    # Create an HDFS client\n",
        "    client = InsecureClient(hdfs_url)\n",
        "\n",
        "    # Get a list of all files in the directory\n",
        "    files = client.list(directory_path, status=True)\n",
        "\n",
        "    # Calculate the total file size\n",
        "    total_size = 0\n",
        "    for file in files:\n",
        "        if not file['isDirectory']:\n",
        "            total_size += file['length']\n",
        "\n",
        "    return total_size\n",
        "\n",
        "\n",
        "# Example usage\n",
        "hdfs_url = 'http://localhost:50070'\n",
        "directory_path = '/user/hadoop/data'\n",
        "total_size = calculate_total_file_size(hdfs_url, directory_path)\n",
        "print(f\"Total File Size: {total_size} bytes\")\n"
      ],
      "metadata": {
        "id": "INLSNwZaUIU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.**"
      ],
      "metadata": {
        "id": "1bONYBOdVvmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import heapq\n",
        "\n",
        "\n",
        "class TopNWords(MRJob):\n",
        "    def configure_args(self):\n",
        "        super(TopNWords, self).configure_args()\n",
        "        self.add_passthru_arg(\n",
        "            \"--N\", type=int, default=10, help=\"Number of top words to retrieve\"\n",
        "        )\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper, combiner=self.combiner, reducer=self.reducer),\n",
        "            MRStep(reducer=self.top_n_reducer)\n",
        "        ]\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        for word in line.split():\n",
        "            yield word.lower(), 1\n",
        "\n",
        "    def combiner(self, word, counts):\n",
        "        yield word, sum(counts)\n",
        "\n",
        "    def reducer(self, word, counts):\n",
        "        yield None, (sum(counts), word)\n",
        "\n",
        "    def top_n_reducer(self, _, word_count_pairs):\n",
        "        N = self.options.N\n",
        "        top_n = heapq.nlargest(N, word_count_pairs)\n",
        "        for count, word in top_n:\n",
        "            yield word, count\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper, combiner=self.combiner, reducer=self.reducer),\n",
        "            MRStep(reducer=self.top_n_reducer)\n",
        "        ]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    TopNWords.run()"
      ],
      "metadata": {
        "id": "MqIpphbvUIXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.**"
      ],
      "metadata": {
        "id": "RDGDdyf_WHU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def check_hadoop_cluster_health(namenode_url):\n",
        "    # Check the health status of the NameNode\n",
        "    namenode_health_url = f\"{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
        "    response = requests.get(namenode_health_url)\n",
        "    namenode_health = response.json()['beans'][0]['State']\n",
        "\n",
        "    print(\"NameNode Health Status:\", namenode_health)\n",
        "\n",
        "    # Check the health status of the DataNodes\n",
        "    datanode_health_url = f\"{namenode_url}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
        "    response = requests.get(datanode_health_url)\n",
        "    datanodes = response.json()['beans']\n",
        "\n",
        "    print(\"DataNode Health Status:\")\n",
        "    for datanode in datanodes:\n",
        "        hostname = datanode['HostAndPort']\n",
        "        health = datanode['LastContact']\n",
        "        print(f\"DataNode: {hostname}, Last Contact: {health} ms\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "namenode_url = 'http://localhost:50070'\n",
        "check_hadoop_cluster_health(namenode_url)\n"
      ],
      "metadata": {
        "id": "PKsMO0k4UIZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Develop a Python program that lists all the files and directories in a specific HDFS path.**"
      ],
      "metadata": {
        "id": "Ckt_smEpWnCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hdfs import InsecureClient\n",
        "\n",
        "def list_hdfs_path(hdfs_url, path):\n",
        "    # Create an HDFS client\n",
        "    client = InsecureClient(hdfs_url)\n",
        "\n",
        "    # List files and directories in the HDFS path\n",
        "    file_list = client.list(path, status=True)\n",
        "\n",
        "    # Print the files and directories\n",
        "    for file in file_list:\n",
        "        file_name = file['path']\n",
        "        is_directory = file['isDirectory']\n",
        "        print(f\"{'[DIR]' if is_directory else '[FILE]'} {file_name}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "hdfs_url = 'http://localhost:50070'\n",
        "path = '/user/hadoop/data'\n",
        "list_hdfs_path(hdfs_url, path)"
      ],
      "metadata": {
        "id": "gLD7hwoUUIcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.**"
      ],
      "metadata": {
        "id": "OuJ3jBjrWzQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def analyze_storage_utilization(namenode_url):\n",
        "    # Get the DataNodes information from the Hadoop REST API\n",
        "    datanodes_url = f\"{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo\"\n",
        "    response = requests.get(datanodes_url)\n",
        "    datanodes = response.json()['beans'][0]['LiveNodes']\n",
        "\n",
        "    # Calculate storage utilization for each DataNode\n",
        "    storage_utilization = {}\n",
        "    for datanode in datanodes:\n",
        "        hostname = datanodes[datanode]['HostName']\n",
        "        capacity = datanodes[datanode]['Capacity']\n",
        "        remaining = datanodes[datanode]['Remaining']\n",
        "        utilization = (capacity - remaining) / capacity * 100\n",
        "        storage_utilization[hostname] = utilization\n",
        "\n",
        "    # Identify the DataNode with the highest storage capacity\n",
        "    highest_capacity_node = max(storage_utilization, key=storage_utilization.get)\n",
        "    highest_capacity = storage_utilization[highest_capacity_node]\n",
        "\n",
        "    # Identify the DataNode with the lowest storage capacity\n",
        "    lowest_capacity_node = min(storage_utilization, key=storage_utilization.get)\n",
        "    lowest_capacity = storage_utilization[lowest_capacity_node]\n",
        "\n",
        "    return storage_utilization, highest_capacity_node, highest_capacity, lowest_capacity_node, lowest_capacity\n",
        "\n",
        "\n",
        "# Example usage\n",
        "namenode_url = 'http://localhost:50070'\n",
        "utilization, highest_node, highest_capacity, lowest_node, lowest_capacity = analyze_storage_utilization(namenode_url)\n",
        "\n",
        "print(\"Storage Utilization:\")\n",
        "for node, utilization in utilization.items():\n",
        "    print(f\"{node}: {utilization}%\")\n",
        "\n",
        "print(\"\\nDataNode with the highest storage capacity:\")\n",
        "print(f\"Node: {highest_node}, Utilization: {highest_capacity}%\")\n",
        "\n",
        "print(\"\\nDataNode with the lowest storage capacity:\")\n",
        "print(f\"Node: {lowest_node}, Utilization: {lowest_capacity}%\")"
      ],
      "metadata": {
        "id": "7wAd2ebcUIer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.**"
      ],
      "metadata": {
        "id": "KiJUxHybXAiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def submit_hadoop_job(resource_manager_url, jar_path, class_name, input_path, output_path):\n",
        "    # Submit the Hadoop job\n",
        "    submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
        "    response = requests.post(submit_url)\n",
        "    application_id = response.json()['application-id']\n",
        "\n",
        "    job_submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/app\"\n",
        "    payload = {\n",
        "        'application-id': application_id,\n",
        "        'application-name': 'Hadoop Job',\n",
        "        'am-container-spec': {\n",
        "            'commands': {\n",
        "                'command': f\"hadoop jar {jar_path} {class_name} {input_path} {output_path}\"\n",
        "            }\n",
        "        },\n",
        "        'application-type': 'MAPREDUCE'\n",
        "    }\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "    response = requests.post(job_submit_url, json=payload, headers=headers)\n",
        "\n",
        "    print(\"Hadoop job submitted.\")\n",
        "    print(\"Application ID:\", application_id)\n",
        "\n",
        "    # Monitor job progress\n",
        "    while True:\n",
        "        status_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/app\"\n",
        "        response = requests.get(status_url)\n",
        "        status = response.json()['app']['finalStatus']\n",
        "        if status in ['SUCCEEDED', 'FAILED', 'KILLED']:\n",
        "            break\n",
        "        time.sleep(5)\n",
        "\n",
        "    # Retrieve job output\n",
        "    if status == 'SUCCEEDED':\n",
        "        output_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/appMaster/log\"\n",
        "        response = requests.get(output_url)\n",
        "        output = response.json()['log']['out']\n",
        "        print(\"Hadoop job completed successfully. Output:\")\n",
        "        print(output)\n",
        "    else:\n",
        "        print(\"Hadoop job failed or was killed.\")\n",
        "\n",
        "# Example usage\n",
        "resource_manager_url = 'http://localhost:8088'\n",
        "jar_path = '/path/to/your/hadoop-job.jar'\n",
        "class_name = 'com.example.HadoopJob'\n",
        "input_path = '/input/path'\n",
        "output_path = '/output/path'\n",
        "\n",
        "submit_hadoop_job(resource_manager_url, jar_path, class_name, input_path, output_path)"
      ],
      "metadata": {
        "id": "SjnhA20xUIiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.**"
      ],
      "metadata": {
        "id": "fZsav3DmXXhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def submit_hadoop_job(resource_manager_url, jar_path, class_name, input_path, output_path, num_containers, container_memory, container_vcores):\n",
        "    # Submit the Hadoop job\n",
        "    submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
        "    response = requests.post(submit_url)\n",
        "    application_id = response.json()['application-id']\n",
        "\n",
        "    job_submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/app\"\n",
        "    payload = {\n",
        "        'application-id': application_id,\n",
        "        'application-name': 'Hadoop Job',\n",
        "        'am-container-spec': {\n",
        "            'commands': {\n",
        "                'command': f\"hadoop jar {jar_path} {class_name} {input_path} {output_path}\"\n",
        "            },\n",
        "            'resource': {\n",
        "                'memory': container_memory,\n",
        "                'vCores': container_vcores\n",
        "            }\n",
        "        },\n",
        "        'application-type': 'MAPREDUCE',\n",
        "        'resource': {\n",
        "            'memory': container_memory,\n",
        "            'vCores': container_vcores,\n",
        "            'instances': num_containers\n",
        "        }\n",
        "    }\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "    response = requests.post(job_submit_url, json=payload, headers=headers)\n",
        "\n",
        "    print(\"Hadoop job submitted.\")\n",
        "    print(\"Application ID:\", application_id)\n",
        "\n",
        "    # Monitor job progress and resource usage\n",
        "    while True:\n",
        "        status_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/app\"\n",
        "        response = requests.get(status_url)\n",
        "        status = response.json()['app']['finalStatus']\n",
        "        resources_used = response.json()['app']['amContainerLogs']['resourcesUsed']\n",
        "\n",
        "        if status in ['SUCCEEDED', 'FAILED', 'KILLED']:\n",
        "            break\n",
        "\n",
        "        print(f\"Resource Usage: {resources_used['memory']}MB Memory, {resources_used['vCores']} vCores\")\n",
        "        time.sleep(5)\n",
        "\n",
        "    # Retrieve job output\n",
        "    if status == 'SUCCEEDED':\n",
        "        output_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/appMaster/log\"\n",
        "        response = requests.get(output_url)\n",
        "        output = response.json()['log']['out']\n",
        "        print(\"Hadoop job completed successfully. Output:\")\n",
        "        print(output)\n",
        "    else:\n",
        "        print(\"Hadoop job failed or was killed.\")\n",
        "\n",
        "# Example usage\n",
        "resource_manager_url = 'http://localhost:8088'\n",
        "jar_path = '/path/to/your/hadoop-job.jar'\n",
        "class_name = 'com.example.HadoopJob'\n",
        "input_path = '/input/path'\n",
        "output_path = '/output/path'\n",
        "num_containers = 2\n",
        "container_memory = 1024  # in MB\n",
        "container_vcores = 1\n",
        "\n",
        "submit_hadoop_job(resource_manager_url, jar_path, class_name, input_path, output_path, num_containers, container_memory, container_vcores)"
      ],
      "metadata": {
        "id": "OjmfTQc3UImk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.**"
      ],
      "metadata": {
        "id": "c4-D1gWlXrWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import time\n",
        "\n",
        "class MapReducePerformance(MRJob):\n",
        "    def configure_args(self):\n",
        "        super(MapReducePerformance, self).configure_args()\n",
        "        self.add_passthru_arg(\"--split-size\", type=int, default=64,\n",
        "                              help=\"Input split size in megabytes\")\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        yield None, line\n",
        "\n",
        "    def reducer(self, _, lines):\n",
        "        time.sleep(1)  # Simulate processing time\n",
        "        for line in lines:\n",
        "            yield _, line\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper, reducer=self.reducer)\n",
        "        ]\n",
        "\n",
        "    def input_protocol(self):\n",
        "        split_size_mb = self.options.split_size\n",
        "        return 'raw_value_split', f'-D mapreduce.job.split.metainfo.maxsize={split_size_mb}M'\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MapReducePerformance.run()"
      ],
      "metadata": {
        "id": "iULb-BFDUIo2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}