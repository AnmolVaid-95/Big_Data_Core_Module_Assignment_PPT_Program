{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Working with RDDs:\n",
        "\n",
        "   a) Write a Python program to create an RDD from a local data source.\n",
        "\n",
        "   b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
        "   \n",
        "   c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n"
      ],
      "metadata": {
        "id": "aHc87-tvhSwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYjJZSHVgUrD"
      },
      "outputs": [],
      "source": [
        "a)\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Creation Example\")\n",
        "\n",
        "# Read data from a local text file\n",
        "data_file = \"path/to/local/file.txt\"\n",
        "rdd = sc.textFile(data_file)\n",
        "\n",
        "# Perform operations on the RDD\n",
        "# For example, print the first 5 lines\n",
        "lines = rdd.take(5)\n",
        "for line in lines:\n",
        "    print(line)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b)\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Data Processing Example\")\n",
        "\n",
        "# Read data from a local text file\n",
        "data_file = \"path/to/local/file.txt\"\n",
        "rdd = sc.textFile(data_file)\n",
        "\n",
        "# Apply transformations\n",
        "# Example 1: Filter lines containing a specific keyword\n",
        "filtered_rdd = rdd.filter(lambda line: \"keyword\" in line)\n",
        "\n",
        "# Example 2: Split each line into words\n",
        "words_rdd = rdd.flatMap(lambda line: line.split())\n",
        "\n",
        "# Apply actions\n",
        "# Example 1: Count the number of lines in the RDD\n",
        "line_count = rdd.count()\n",
        "print(\"Number of lines:\", line_count)\n",
        "\n",
        "# Example 2: Count the number of words in the RDD\n",
        "word_count = words_rdd.count()\n",
        "print(\"Number of words:\", word_count)\n",
        "\n",
        "# Example 3: Collect the RDD elements as a list\n",
        "lines = rdd.collect()\n",
        "print(\"RDD elements:\")\n",
        "for line in lines:\n",
        "    print(line)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "GkDmEKX3hRMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c)\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Operations Example\")\n",
        "\n",
        "# Create an RDD with sample data\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Use map operation to perform a transformation\n",
        "squared_rdd = rdd.map(lambda x: x**2)\n",
        "print(\"Squared RDD:\", squared_rdd.collect())\n",
        "\n",
        "# Use filter operation to apply a condition\n",
        "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
        "print(\"Even RDD:\", even_rdd.collect())\n",
        "\n",
        "# Use reduce operation to compute a sum\n",
        "sum_result = rdd.reduce(lambda x, y: x + y)\n",
        "print(\"Sum:\", sum_result)\n",
        "\n",
        "# Use aggregate operation to compute sum and count simultaneously\n",
        "sum_count = rdd.aggregate((0, 0), lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
        "                          lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
        "print(\"Sum:\", sum_count[0])\n",
        "print(\"Count:\", sum_count[1])\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "iiaVOBj5hRPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Spark DataFrame Operations:\n",
        "   \n",
        "   a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
        "   \n",
        "   b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
        "   \n",
        "   c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n"
      ],
      "metadata": {
        "id": "KnfmWKMThxTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a)\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"CSV to DataFrame\").getOrCreate()\n",
        "\n",
        "# Load CSV file into a DataFrame\n",
        "csv_file = \"path/to/csv/file.csv\"\n",
        "df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
        "\n",
        "# Display the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "SOXhyIVbhRRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b)\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrame Operations\").getOrCreate()\n",
        "\n",
        "# Load data into a DataFrame\n",
        "data = [\n",
        "    (\"Alice\", 25, \"London\"),\n",
        "    (\"Bob\", 30, \"New York\"),\n",
        "    (\"Charlie\", 35, \"London\"),\n",
        "    (\"Diana\", 28, \"San Francisco\"),\n",
        "    (\"Eve\", 33, \"London\")\n",
        "]\n",
        "columns = [\"Name\", \"Age\", \"City\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Display the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Filter data using a condition\n",
        "filtered_df = df.filter(col(\"Age\") > 30)\n",
        "filtered_df.show()\n",
        "\n",
        "# Group data by a column and compute aggregate functions\n",
        "grouped_df = df.groupBy(\"City\").agg({\"Age\": \"avg\", \"Name\": \"count\"})\n",
        "grouped_df.show()\n",
        "\n",
        "# Join two DataFrames based on a common column\n",
        "other_data = [\n",
        "    (\"London\", \"UK\"),\n",
        "    (\"New York\", \"USA\"),\n",
        "    (\"San Francisco\", \"USA\")\n",
        "]\n",
        "other_columns = [\"City\", \"Country\"]\n",
        "other_df = spark.createDataFrame(other_data, other_columns)\n",
        "\n",
        "joined_df = df.join(other_df, \"City\")\n",
        "joined_df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "o3E_ezYRhRUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c)\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark SQL Queries\").getOrCreate()\n",
        "\n",
        "# Load data into a DataFrame\n",
        "data = [\n",
        "    (\"Alice\", 25, \"London\"),\n",
        "    (\"Bob\", 30, \"New York\"),\n",
        "    (\"Charlie\", 35, \"London\"),\n",
        "    (\"Diana\", 28, \"San Francisco\"),\n",
        "    (\"Eve\", 33, \"London\")\n",
        "]\n",
        "columns = [\"Name\", \"Age\", \"City\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Register the DataFrame as a temporary view\n",
        "df.createOrReplaceTempView(\"my_table\")\n",
        "\n",
        "# Execute Spark SQL queries\n",
        "query1 = \"SELECT * FROM my_table WHERE Age > 30\"\n",
        "result1 = spark.sql(query1)\n",
        "result1.show()\n",
        "\n",
        "query2 = \"SELECT City, COUNT(*) AS Count FROM my_table GROUP BY City\"\n",
        "result2 = spark.sql(query2)\n",
        "result2.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "mNZOfVwyhRW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Spark Streaming:\n",
        "\n",
        "  a) Write a Python program to create a Spark Streaming application.\n",
        "\n",
        "   b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
        "\n",
        "   c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n"
      ],
      "metadata": {
        "id": "vGU3RT5-iXHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a)\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local[2]\", \"Spark Streaming Application\")\n",
        "\n",
        "# Create a StreamingContext with a batch interval of 1 second\n",
        "ssc = StreamingContext(sc, 1)\n",
        "\n",
        "# Create a DStream by subscribing to a TCP socket and listening on localhost:9999\n",
        "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "# Perform operations on the DStream\n",
        "word_counts = lines.flatMap(lambda line: line.split()) \\\n",
        "                  .map(lambda word: (word, 1)) \\\n",
        "                  .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the word counts\n",
        "word_counts.pprint()\n",
        "\n",
        "# Start the streaming context\n",
        "ssc.start()\n",
        "\n",
        "# Wait for the streaming to finish\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "u7YklWvUhRZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b)\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local[2]\", \"Spark Streaming Application\")\n",
        "\n",
        "# Create a StreamingContext with a batch interval of 1 second\n",
        "ssc = StreamingContext(sc, 1)\n",
        "\n",
        "# Configure Kafka parameters\n",
        "kafka_params = {\n",
        "    \"bootstrap.servers\": \"localhost:9092\",  # Kafka broker(s) address\n",
        "    \"group.id\": \"my_consumer_group\",        # Consumer group ID\n",
        "    \"auto.offset.reset\": \"latest\"           # Start consuming from the latest offset\n",
        "}\n",
        "\n",
        "# Create a DStream by consuming from a Kafka topic\n",
        "topic = \"my_topic\"\n",
        "dstream = KafkaUtils.createDirectStream(ssc, [topic], kafkaParams=kafka_params)\n",
        "\n",
        "# Perform operations on the DStream\n",
        "lines = dstream.map(lambda x: x[1])  # Extract the value from Kafka message\n"
      ],
      "metadata": {
        "id": "6VhwOb8mhRb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c)\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local[2]\", \"Spark Streaming Application\")\n",
        "\n",
        "# Create a StreamingContext with a batch interval of 1 second\n",
        "ssc = StreamingContext(sc, 1)\n",
        "\n",
        "# Configure Kafka parameters\n",
        "kafka_params = {\n",
        "    \"bootstrap.servers\": \"localhost:9092\",  # Kafka broker(s) address\n",
        "    \"group.id\": \"my_consumer_group\",        # Consumer group ID\n",
        "    \"auto.offset.reset\": \"latest\"           # Start consuming from the latest offset\n",
        "}\n",
        "\n",
        "# Create a DStream by consuming from a Kafka topic\n",
        "topic = \"my_topic\"\n",
        "dstream = KafkaUtils.createDirectStream(ssc, [topic], kafkaParams=kafka_params)\n",
        "\n",
        "# Perform streaming transformations and actions\n",
        "lines = dstream.map(lambda x: x[1])  # Extract the value from Kafka message\n",
        "\n",
        "# Transformation: Word count\n",
        "word_counts = lines.flatMap(lambda line: line.split()) \\\n",
        "                  .map(lambda word: (word, 1)) \\\n",
        "                  .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Action: Print the word counts\n",
        "word_counts.pprint()\n",
        "\n",
        "# Transformation: Filter\n",
        "filtered_lines = lines.filter(lambda line: line.startswith(\"Error\"))\n",
        "\n",
        "# Action: Print the filtered lines\n",
        "filtered_lines.pprint()\n",
        "\n",
        "# Transformation: Window\n",
        "windowed_word_counts = lines.window(windowDuration=10, slideDuration=5) \\\n",
        "                             .flatMap(lambda line: line.split()) \\\n",
        "                             .map(lambda word: (word, 1)) \\\n",
        "                             .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Action: Print the windowed word counts\n",
        "windowed_word_counts.pprint()\n",
        "\n",
        "# Start the streaming context\n",
        "ssc.start()\n",
        "\n",
        "# Wait for the streaming to finish\n",
        "ssc.awaitTermination()"
      ],
      "metadata": {
        "id": "OClg1SZvhReS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Spark SQL and Data Source Integration:\n",
        "\n",
        "   a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
        "\n",
        "   b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
        "   \n",
        "   c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.\n"
      ],
      "metadata": {
        "id": "ZbbGZ6L0i3Xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a)\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark-MySQL Connection\").getOrCreate()\n",
        "\n",
        "# Configure MySQL connection properties\n",
        "mysql_host = \"localhost\"\n",
        "mysql_port = \"3306\"\n",
        "mysql_database = \"your_database\"\n",
        "mysql_username = \"your_username\"\n",
        "mysql_password = \"your_password\"\n",
        "\n",
        "# Configure JDBC URL for MySQL\n",
        "jdbc_url = f\"jdbc:mysql://{mysql_host}:{mysql_port}/{mysql_database}\"\n",
        "\n",
        "# Configure connection properties\n",
        "connection_properties = {\n",
        "    \"user\": mysql_username,\n",
        "    \"password\": mysql_password,\n",
        "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
        "}\n",
        "\n",
        "# Read data from MySQL table into a DataFrame\n",
        "df = spark.read.jdbc(url=jdbc_url, table=\"your_table\", properties=connection_properties)\n",
        "\n",
        "# Display the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "OWEVB-9WhRgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b)\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark SQL Operations\").getOrCreate()\n",
        "\n",
        "# Configure MySQL connection properties\n",
        "mysql_host = \"localhost\"\n",
        "mysql_port = \"3306\"\n",
        "mysql_database = \"your_database\"\n",
        "mysql_username = \"your_username\"\n",
        "mysql_password = \"your_password\"\n",
        "\n",
        "# Configure JDBC URL for MySQL\n",
        "jdbc_url = f\"jdbc:mysql://{mysql_host}:{mysql_port}/{mysql_database}\"\n",
        "\n",
        "# Configure connection properties\n",
        "connection_properties = {\n",
        "    \"user\": mysql_username,\n",
        "    \"password\": mysql_password,\n",
        "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
        "}\n",
        "\n",
        "# Read data from MySQL table into a DataFrame\n",
        "df = spark.read.jdbc(url=jdbc_url, table=\"your_table\", properties=connection_properties)\n",
        "\n",
        "# Create a temporary view of the DataFrame\n",
        "df.createOrReplaceTempView(\"my_table\")\n",
        "\n",
        "# Execute SQL queries on the data\n",
        "query1 = \"SELECT * FROM my_table WHERE column1 = 'value'\"\n",
        "result1 = spark.sql(query1)\n",
        "result1.show()\n",
        "\n",
        "query2 = \"SELECT column2, COUNT(*) AS count FROM my_table GROUP BY column2\"\n",
        "result2 = spark.sql(query2)\n",
        "result2.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "YT5EClnkhRjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c)\n",
        "'''\n",
        "Spark offers excellent integration capabilities with various data sources, including Hadoop Distributed File System (HDFS) and Amazon S3. Here's an overview of how Spark can interact with these data sources:\n",
        "\n",
        "Hadoop Distributed File System (HDFS):\n",
        "\n",
        "Reading from HDFS: Spark can read data from HDFS using the SparkContext or SparkSession APIs. You can use textFile(), wholeTextFiles(), or binaryFiles() methods to read text, multiple text files, or binary files, respectively.\n",
        "Writing to HDFS: Spark provides methods like saveAsTextFile() or saveAsObjectFile() to save RDDs or DataFrames to HDFS. You can specify the output path within HDFS.\n",
        "Amazon S3:\n",
        "\n",
        "Reading from S3: Spark supports reading data from Amazon S3 by providing the S3 bucket path as the file input. You can use methods like textFile(), csv(), json(), or parquet() to read the corresponding file formats from S3.\n",
        "Writing to S3: Spark allows writing RDDs or DataFrames to S3 by specifying the S3 bucket path as the output location. You can use methods like saveAsTextFile(), write.csv(), write.json(), or write.parquet() to save the data in the desired format.\n",
        "To interact with HDFS or S3, you need to configure the appropriate Hadoop or AWS credentials in the Spark configuration. For HDFS, you might set HADOOP_CONF_DIR or spark.hadoop.* properties. For S3, you need to provide AWS access key, secret key, and region using spark.hadoop.fs.s3a.access.key, spark.hadoop.fs.s3a.secret.key, and spark.hadoop.fs.s3a.region properties, respectively.\n",
        "\n",
        "Here's an example of how to read data from HDFS and write it to S3 using PySpark:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"HDFS to S3 Integration\").getOrCreate()\n",
        "\n",
        "# Read data from HDFS\n",
        "hdfs_path = \"hdfs://localhost:9000/path/to/input\"\n",
        "df = spark.read.csv(hdfs_path, header=True, inferSchema=True)\n",
        "\n",
        "# Perform operations on the DataFrame\n",
        "# ...\n",
        "\n",
        "# Write data to S3\n",
        "s3_bucket = \"s3://your-bucket/path/to/output\"\n",
        "df.write.csv(s3_bucket, mode=\"overwrite\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n",
        "In this example, we read data from HDFS using spark.read.csv() and specify the HDFS path. Then, we perform operations on the DataFrame as needed. Finally, we write the DataFrame to S3 using df.write.csv() and specify the S3 bucket path.\n",
        "\n",
        "Ensure that you have the necessary permissions and credentials to access HDFS or S3, and adjust the paths and configurations accordingly.\n",
        "\n",
        "With Spark's flexibility and support for various data sources, you can seamlessly integrate with HDFS, S3, and other data storage systems to process and analyze data efficiently.\n",
        "'''\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"HDFS to S3 Integration\").getOrCreate()\n",
        "\n",
        "# Read data from HDFS\n",
        "hdfs_path = \"hdfs://localhost:9000/path/to/input\"\n",
        "df = spark.read.csv(hdfs_path, header=True, inferSchema=True)\n",
        "\n",
        "# Perform operations on the DataFrame\n",
        "# ...\n",
        "\n",
        "# Write data to S3\n",
        "s3_bucket = \"s3://your-bucket/path/to/output\"\n",
        "df.write.csv(s3_bucket, mode=\"overwrite\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "1gC0J3nLhRlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zvelAu1phRn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "soXYshCFhRs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwYNqJmjhRwb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}